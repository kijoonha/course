{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCYO6dmGgefe"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er0RD438gRLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755ab7c6-a2ca-4196-9f5e-c39de97cb676"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfeql_8sgnKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c882deb4-c122-44e1-cf21-8cfc165ff50a"
      },
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "#%cd 'COPY&PASTE FILE DIRECTORY HERE'\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPEoabX-hGCh"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyammZP8hI7P"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist.data_utils import load_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxTNOvI5NHD"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuQB6W2U5ZE2"
      },
      "source": [
        "def leaky_relu(z):\n",
        "    \"\"\"\n",
        "    Implement the leaky ReLU activation function.\n",
        "    The method takes the input z and returns the output of the function.\n",
        "    \n",
        "    Set the value of alpha for the leaky ReLU funtion to 0.01.\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    #### YOUR CODE #####\n",
        "    return np.maximum(0.01*z, z)\n",
        "    #####################\n",
        "\n",
        "def softmax(X):\n",
        "    \"\"\"\n",
        "    Implement the softmax function.\n",
        "    The method takes the input X and returns the output of the function.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE #####\n",
        "    exp_X = np.exp(X)\n",
        "    sum_exp_X = np.sum(exp_X, axis=1, keepdims=True)\n",
        "\n",
        "    return exp_X/sum_exp_X\n",
        "    #####################\n",
        "\n",
        "def load_batch(X, Y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates batches with the remainder dropped.\n",
        "\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        permutation = np.random.permutation(X.shape[0])\n",
        "        X = X[permutation, :]\n",
        "        Y = Y[permutation, :]\n",
        "    num_steps = int(X.shape[0])//batch_size\n",
        "    step = 0\n",
        "    while step<num_steps:\n",
        "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
        "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
        "        step+=1\n",
        "        yield X_batch, Y_batch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsU8v_6khR30"
      },
      "source": [
        "#2-Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA5udiGmhRb5"
      },
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\" a neural network with 2 layers \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        Do NOT modify this function.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_classes = num_classes\n",
        "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
        "\n",
        "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        initializes parameters with He Initialization.\n",
        "\n",
        "        Question (b)\n",
        "        - refer to https://paperswithcode.com/method/he-initialization for He initialization \n",
        "        \n",
        "        Inputs\n",
        "        - input_dim\n",
        "        - num_hiddens\n",
        "        - num_classes\n",
        "        Returns\n",
        "        - params: a dictionary with the initialized parameters.\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        ##### YOUR CODE #####\n",
        "        params[\"W1\"] = np.random.randn(input_dim, num_hiddens)*np.sqrt(2/input_dim)\n",
        "        params[\"b1\"] = np.zeros((1,num_hiddens))  \n",
        "        params[\"W2\"] = np.random.randn(num_hiddens, num_classes)*np.sqrt(2/num_hiddens)\n",
        "        params[\"b2\"] = np.zeros((1,num_classes))\n",
        "        #####################\n",
        "        \n",
        "        return params\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Define and perform the feed forward step of a two-layer neural network.\n",
        "        Specifically, the network structue is given by\n",
        "\n",
        "          y = softmax(leaky_relu(X W1 + b1) W2 + b2)\n",
        "\n",
        "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "\n",
        "        Question (c)\n",
        "        - ff_dict will be used to run backpropagation in backward method.\n",
        "\n",
        "        Inputs\n",
        "        - X: the input matrix of shape (N, D)\n",
        "\n",
        "        Returns\n",
        "        - y: the output of the model\n",
        "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
        "        \"\"\"\n",
        "        ff_dict = {}\n",
        "        ##### YOUR CODE #####\n",
        "        W1 = self.params[\"W1\"]\n",
        "        b1 = self.params[\"b1\"]\n",
        "        Z1 = np.matmul(X, W1)+b1\n",
        "        \n",
        "        A1 = leaky_relu(Z1)\n",
        "        W2 = self.params[\"W2\"]\n",
        "        b2 = self.params[\"b2\"]\n",
        "        Z2 = np.matmul(A1, W2)+ b2\n",
        "        y = softmax(Z2)\n",
        "        ff_dict[\"Z1\"]= Z1\n",
        "        ff_dict[\"A1\"] = A1\n",
        "        ff_dict[\"Z2\"]= Z2\n",
        "        ff_dict[\"y\"] = y\n",
        "        ff_dict[\"W2\"]=W2\n",
        "        #####################        \n",
        "        return y, ff_dict\n",
        "\n",
        "    def backward(self, X, Y, ff_dict):\n",
        "        \"\"\"\n",
        "        Performs backpropagation over the two-layer neural network, and returns\n",
        "        a dictionary of gradients of all model parameters.\n",
        "\n",
        "        Question (d)\n",
        "\n",
        "        Inputs:\n",
        "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "              in a mini-batch, D is the feature dimensionality.\n",
        "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "              where B is the number of examples in a mini-batch, C is the number\n",
        "              of classes.\n",
        "         - ff_dict: the dictionary containing all the fully connected units and\n",
        "              activations.\n",
        "\n",
        "        Returns:\n",
        "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
        "        \"\"\"\n",
        "        grads = {}\n",
        "        ##### YOUR CODE #####\n",
        "        m = X.shape[0]\n",
        "        dZ2 = ff_dict[\"y\"] - Y\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(ff_dict[\"A1\"].T,dZ2) \n",
        "        dA1 = np.dot(dZ2, ff_dict[\"W2\"].T)\n",
        "\n",
        "        #leaky relu = dZ1계산\n",
        "        dx = np.ones_like(dA1)\n",
        "        dx[ff_dict[\"Z1\"]<0] = 0.01 \n",
        "        dZ1 = dA1 * dx \n",
        "\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "        dW1 = np.matmul(X.T, dZ1)\n",
        "\n",
        "        grads[\"dW1\"] = dW1 \n",
        "        grads[\"db1\"] = db1  \n",
        "        grads[\"dW2\"] = dW2\n",
        "        grads[\"db2\"] = db2\n",
        "        #####################\n",
        "        \n",
        "        return grads\n",
        "\n",
        "    def compute_loss(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes cross entropy loss.\n",
        "\n",
        "        Do NOT modify this function.\n",
        "\n",
        "        Inputs\n",
        "            Y:\n",
        "            Y_hat:\n",
        "        Returns\n",
        "            loss:\n",
        "        \"\"\"\n",
        "        epsilon = 1e-10\n",
        "        Y_hat = np.clip(Y_hat, epsilon, 1 - epsilon)\n",
        "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
        "        \"\"\"\n",
        "        Runs mini-batch gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X\n",
        "        - Y\n",
        "        - X_val\n",
        "        - Y_Val\n",
        "        - lr\n",
        "        - n_epochs\n",
        "        - batch_size\n",
        "        - log_interval\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
        "                self.train_step(X_batch, Y_batch, batch_size, lr)\n",
        "            if epoch % log_interval==0:\n",
        "                Y_hat, ff_dict = self.forward(X)\n",
        "                train_loss = self.compute_loss(Y, Y_hat)\n",
        "                train_acc = self.evaluate(Y, Y_hat)\n",
        "                Y_hat, ff_dict = self.forward(X_val)\n",
        "                valid_loss = self.compute_loss(Y_val, Y_hat)\n",
        "                valid_acc = self.evaluate(Y_val, Y_hat)\n",
        "                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
        "                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "\n",
        "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
        "        \"\"\"\n",
        "        Updates the parameters using gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X_batch\n",
        "        - Y_batch\n",
        "        - batch_size\n",
        "        - lr\n",
        "        \"\"\"\n",
        "        _, ff_dict = self.forward(X_batch)\n",
        "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
        "        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n",
        "        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n",
        "        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n",
        "        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n",
        "\n",
        "    def evaluate(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes classification accuracy.\n",
        "        \n",
        "        Do NOT modify this function\n",
        "\n",
        "        Inputs\n",
        "        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n",
        "             where C is the number of classes.\n",
        "        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
        "             where C is the number of classes.\n",
        "\n",
        "        Returns\n",
        "            accuracy: the classification accuracy in float\n",
        "        \"\"\"        \n",
        "        classes_pred = np.argmax(Y_hat, axis=1)\n",
        "        classes_gt = np.argmax(Y, axis=1)\n",
        "        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n",
        "        return accuracy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXM2lWhtDYC6"
      },
      "source": [
        "#Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ooR6YIxYhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd2dd6a-7fe8-4d49-99ca-39327e1efd75"
      },
      "source": [
        "X_train, Y_train, X_test, Y_test = load_data()\n",
        "\n",
        "idxs = np.arange(len(X_train))\n",
        "np.random.shuffle(idxs)\n",
        "split_idx = int(np.ceil(len(idxs)*0.8))\n",
        "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
        "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
        "print()\n",
        "print('Set validation data aside')\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', Y_train.shape)\n",
        "print('Validation data shape: ', X_valid.shape)\n",
        "print('Validation labels shape: ', Y_valid.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded:\n",
            "Training data shape: (60000, 784)\n",
            "Training labels shape: (60000, 10)\n",
            "Test data shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "\n",
            "Set validation data aside\n",
            "Training data shape:  (48000, 784)\n",
            "Training labels shape:  (48000, 10)\n",
            "Validation data shape:  (12000, 784)\n",
            "Validation labels shape:  (12000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzw-D4Zr5xoi"
      },
      "source": [
        "#Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlnC_rerHPaN"
      },
      "source": [
        "### \n",
        "# Question (e)\n",
        "# Tune the hyperparameters with validation data, \n",
        "# and print the results by running the lines below.\n",
        "###"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCqVT4S0Tm5"
      },
      "source": [
        "# model instantiation\n",
        "model = TwoLayerNN(input_dim=784, num_hiddens=64, num_classes=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cWb6xg0NxOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20d2411-6fd8-4231-b784-df43f08fa490"
      },
      "source": [
        "# train the model\n",
        "lr, n_epochs, batch_size = 0.01, 10, 128\n",
        "\n",
        "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 - train loss/acc: 0.308 0.912, valid loss/acc: 0.316 0.910\n",
            "epoch 01 - train loss/acc: 0.244 0.931, valid loss/acc: 0.255 0.927\n",
            "epoch 02 - train loss/acc: 0.209 0.942, valid loss/acc: 0.222 0.938\n",
            "epoch 03 - train loss/acc: 0.188 0.946, valid loss/acc: 0.205 0.941\n",
            "epoch 04 - train loss/acc: 0.162 0.954, valid loss/acc: 0.177 0.950\n",
            "epoch 05 - train loss/acc: 0.149 0.958, valid loss/acc: 0.165 0.953\n",
            "epoch 06 - train loss/acc: 0.136 0.961, valid loss/acc: 0.156 0.954\n",
            "epoch 07 - train loss/acc: 0.126 0.965, valid loss/acc: 0.146 0.958\n",
            "epoch 08 - train loss/acc: 0.117 0.967, valid loss/acc: 0.138 0.959\n",
            "epoch 09 - train loss/acc: 0.107 0.970, valid loss/acc: 0.132 0.962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpPsAlXU0T_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22895928-415e-47ab-8b85-15a11e8c363f"
      },
      "source": [
        "# evalute the model on test data\n",
        "Y_hat, _ = model.forward(X_test)\n",
        "test_loss = model.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test loss = 0.122, acc = 0.963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#결과분석\n",
        "# learning rate을 0.01로 조정하고 epoch 수는 10, batch_size는 128로 설정하였을때\n",
        "# final acc가 0.963d으로 높게 나옴\n",
        "# learnig rate이 2일 경우에는 학습이 제대로 되지 않기 때문에 낮게 조정해야함.\n"
      ],
      "metadata": {
        "id": "XzN0kjjx3jUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmmS6Cs18xlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}